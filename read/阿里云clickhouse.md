参考 [云数据库ClickHouse：从云托管到云原生背后的核心技术解析](https://mp.weixin.qq.com/s/1tj35tUrlPFbzv5o3otTHA)

>云托管模式把用户从手动部署运维的时代中解放了出来。而云原生数据库面向用户最核心的一个体感就是要提供更好的资源弹性能力、数据共享能力。从数据库引擎内核层面来看，云原生数据库需要更好地去拥抱利用云平台上的基础设施能力，一些传统的数据库架构模式是基于本地不可靠存储设备、有限计算资源来设计的，在云平台这个大前提下将不再是最优选择

第一个比较有价值的观点，上云之后，很多本地数据库的设计假设是会被打破的
因此需要先了解云是什么？云可以带来什么？


>但是作者认为云原生版ClickHouse仍然不能抛弃掉数据本身的sharding，因为ClickHouse的Merge存储还是存在部分主键语义的(其实就是它的Order By Key)，主要表现在ReplacingMergeTree/AggregatingMergeTree等变种引擎中。ClickHouse提供了主键最终一致性（optimize partition）的能力，也有主键读时一致性（final scan）的能力

第二个是关于是否需要shard，其实也就是分桶

作者认为需要保留shard的原因，现有ck设计模型的性能会有所下降，对ck的各种数据结构的模型不太了解，这块可能需要在深入调研下
其实从doris的视角看，也是有类似的问题，比如当支持存算分离之后，查询的吞吐其实会上升很多，那么doris的分桶逻辑是否还必要呢？
先说分桶的好处
1 对数据做裁剪，查询性能会有锁提升
2 其他的分桶内排序，预聚合的逻辑，感觉不用分桶，单用分片的方式可以实现；但是性能可能还是谁有所下降，这块需要重新梳理查询逻辑

> 单节点多shard
这个设计没看很明白
之前是只能通过分布式表才可以访问到shard
现在似乎是可以直接访问对应的shard？

> 一写多读 vs 多写多读

目前ck的merge表是支持多写多读的。但是doris其实是串行写的
如果后续支持存算分离的话，一写多读和多写多读确实是个需要思考的问题
元数据由去中心化到中心化，这块比较好奇的是去中心化的元数据是如何维护的，这个需要进一步调研

MergeTree的数据共享
mergetree的datapart是不可变的，然后用mutation代表了mergetree的异步变更
而用commitid对齐进行版本的管理

一写的问题，写本地的数据无法立即可见
修改表查询的内部路由策略


多读的问题，多个读计算组如何获取变更
全量+增量同步变更，我感觉这里的问题是，这种被动同步的方式，分布式一致性如何解决
节点之间同步数据的速度总是会有差异的，有没有可能同一个查询落到不同的节点，看到的数据不一样？


> 对象存储加速

ck的常见操作，rename，hardlink，listDir等操作如果直接在对象存储上去做模拟，性能开销不可接受
优化方法
1 针对常规的目录操作，ck请求controller来完成操作
2 CK内核会直接和对象存储文件交互